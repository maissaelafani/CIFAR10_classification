# -*- coding: utf-8 -*-
"""TP_ML (4).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SG3kbBfKrpXaCCfKY_-QqI0fwIN_YxDF

# 1. The dataset : CIFAR-3
"""

import numpy as np
X = np.load('X_cifar.npy')
print(np.shape(X))

import numpy as np
X_grayscale = np.load('X_cifar_grayscale.npy')
print(np.shape(X_grayscale))

Y = np.load('Y_cifar.npy')

# Plot a picture
import matplotlib.pyplot as plt

nb_sample = 1
plt.imshow(X[nb_sample])
img_title = 'Classe ' + str(Y[nb_sample])
plt.title(img_title)
plt.show()
plt.clf()

# Plot a gray picture

nb_sample = 1
plt.imshow(X_grayscale[nb_sample], cmap='gray')
img_title = 'Classe ' + str(Y[nb_sample])
plt.title(img_title)
plt.show()
plt.clf()

random_indices = np.random.choice(len(X), size=6, replace=False)

fig, axes = plt.subplots(2, 3, figsize=(10, 6))

for i, ax in enumerate(axes.flatten()):
    img_index = random_indices[i]
    ax.imshow(X[img_index])
    ax.set_title(f"Image {img_index}")
    ax.axis('off')

plt.tight_layout()
plt.show()

"""Q1 : What are the shape of the data?

Each images are 32x32 pixels.

What are the min and max value of the pixels?
"""

X_reshaped = X.reshape(-1, 3)

min_values = X_reshaped.min(axis=0)
max_values = X_reshaped.max(axis=0)
print(f"min = {min_values}; max = {max_values}")

"""The min value of the pixels is 0, and the max value is 255.

Is it better to normalize the data to work in [0,1]?

?

Display samples from the dataset.
"""

nb_sample_1 = 4381
nb_sample_2 = 8326

plt.subplot(1, 2, 1)
plt.imshow(X[nb_sample_1])
img_title_1 = 'Image number ' + str(nb_sample_1)
plt.title(img_title_1)

plt.subplot(1, 2, 2)
plt.imshow(X[nb_sample_2])
img_title_2 = 'Image number ' + str(nb_sample_2)
plt.title(img_title_2)

plt.show()
plt.clf()

nb_sample_1 = 12076
nb_sample_2 = 1529

plt.subplot(1, 2, 1)
plt.imshow(X_grayscale[nb_sample_1], cmap='gray')
img_title_1 = 'Image number ' + str(nb_sample_1)
plt.title(img_title_1)

plt.subplot(1, 2, 2)
plt.imshow(X_grayscale[nb_sample_2], cmap='gray')
img_title_2 = 'Image number ' + str(nb_sample_2)
plt.title(img_title_2)

plt.show()
plt.clf()

"""Q2 : Use the sklearn method train_test_split to split the dataset in one train set and one test set."""

from sklearn.model_selection import train_test_split

# We use 20% of the dataset for the test set
test_size = 0.2

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=42)

print("Train set size: ", len(X_train))
print("Test set size: ", len(X_test))

"""Why this split is important in Machine Learning?

It is important to split the dataset to evaluate the performance of the modele, to avoid overfitting and to detect biais.

Q3 : Are the train and test sets well balanced (distribution of labels)?
"""

# Fonction pour afficher la distribution des classes
def print_class_distribution(labels, dataset_name):
    unique_classes, class_counts = np.unique(labels, return_counts=True)
    total_samples = len(labels)
    print(f"\nDistribution des classes dans l'ensemble {dataset_name}:")
    for class_label, count in zip(unique_classes, class_counts):
        percentage = (count / total_samples) * 100
        print(f"Classe {class_label}: {count} échantillons ({percentage:.2f}%)")

# Affiche la distribution des classes pour l'ensemble d'entraînement
print_class_distribution(Y_train, "d'entraînement")

# Affiche la distribution des classes pour l'ensemble de test
print_class_distribution(Y_test, "de test")

"""The maximum deviation for the train set is 0.25%, and 1.41% for the test set. We can therefore say that the two sets are well balanced.

Why is it important for supervised Machine Learning?

Avoid overfit and maybe other things.

# 2. DIMENSIONALITY REDUCTION WITH THE PCA

Q1 : Perform a Principal Component Analysis (PCA) with sklearn. You will need to reshape.
Try to keep different n_components.

Voir le code ci-dessous.
"""

from sklearn.decomposition import PCA

# On redimensionne le dataset
X_gray_reshaped = np.reshape(X_grayscale, (18000, 32 * 32))

# On initialise quelques valeurs de n_components
n_components_values = [500, 300, 200, 100, 50, 20, 10]

# On initialise l'objet avec le nom de composantes que l'on souhaite
pca = PCA(n_components=n_components_values[0])

# On applique le PCA sur le dataset
X_pca = pca.fit_transform(X_gray_reshaped)

"""Q2 : An interesting feature is PCA.explained_variance_ratio_. Explain these values according to your understanding of PCA and use these values to fit a relevant value for n_components.

PCA.explained_variance_ratio_ returns the ratio of variance explained by each of the components. These values are the proportion of the dataset's total variance that is captured by each component. By adding more components, the reduced space is able to explain more variance, because we get this variance by adding the ratios.
Choosing a variance of 95% should be enough to well represent the dataset.
(Insérer le graphique)
"""

# Stockage des résultats
total_explained_variance_values = []

for n_components in n_components_values:
    pca = PCA(n_components=n_components)
    X_pca = pca.fit_transform(X_gray_reshaped)

    # On calcule la variance totale expliquée
    total_explained_variance = np.sum(pca.explained_variance_ratio_)

    # On affiche la variance totale expliquée pour chaque n_components
    print(f"For n_components={n_components}, Total explained variance: {total_explained_variance}")

    # Stockage les résultats
    total_explained_variance_values.append(total_explained_variance)

# Affichage du graphique
plt.plot(n_components_values, total_explained_variance_values, marker='x')
plt.xlabel('Number of Components')
plt.ylabel('Total Explained Variance')
plt.title('Total Explained Variance vs. Number of Components')
plt.show()

"""Q3 : Display a CIFAR-3-GRAY picture with 5 values of n_components"""

n_components_values = [500, 300, 200, 100, 50]

# Nombre d'images par ligne
images_per_row = 3

# Afficher l'image d'origine
plt.subplot(2, images_per_row, 1)
plt.imshow(X_grayscale[6043], cmap='gray')
plt.title('Original')
plt.axis('off')

# Afficher les images transformées par PCA
for i, n_components in enumerate(n_components_values):
    pca = PCA(n_components=n_components)
    X_pca = pca.fit_transform(X_gray_reshaped)

    # Reshape pour revenir à la forme (18000, 32, 32)
    X_restored = pca.inverse_transform(X_pca)
    X_restored = X_restored.reshape((18000, 32, 32))

    # Afficher les images transformées
    plt.subplot(2, images_per_row, i + 2)
    plt.imshow(X_restored[6043], cmap='gray')
    plt.title(f'n_components={n_components}')
    plt.axis('off')

plt.show()

"""# 3.1. Logistic Regression & Gaussian Naïve Bayes Classifier

Q1 : What is the major difference between Naïve Bayes Classifier and Logistic Regression? (check course #2, a clue: what are we trying to predict?)

Pour Bayes on s'intéresse à P(X|Y), alors que pour la plupart des autres méthodes, comme Logistic Regression, on essaie de prédire P(Y|X).

Q2 : With sklearn, train your LR and NBC models. Comment the results. If any, check how to modify the parameters and comment how it influences the results.
"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Reshape X_grayscale to (18000, 32*32) as Logistic Regression expects 2D input
X_reshaped = X_grayscale.reshape((X_grayscale.shape[0], -1))

# Split the dataset into train and test sets
test_size = 0.2
X_train, X_test, Y_train, Y_test = train_test_split(X_reshaped, Y, test_size=test_size, random_state=42)

# Initialize the logistic regression model
model = LogisticRegression(max_iter=5000, solver='saga', random_state=42)

# Train the model
model.fit(X_train, Y_train)

# Make predictions on the test set
Y_pred = model.predict(X_test)

# Evaluate accuracy
accuracy = accuracy_score(Y_test, Y_pred)
print(f"Accuracy: {accuracy}")

"""quand le paramètre max_iter était à 1000, le modèle ne convergeait pas, mais en l'augmentant à 5000 et en changeant le solver par défault par 'saga', le modèle converge. Par contre, on obtient pas d'amélioration significative sur l'accuracy : 0.564722 dans le premier cas contre 0.56722 dans le second."""

from sklearn.naive_bayes import GaussianNB

# Initialize the Naive Bayes model
model = GaussianNB()

# Train the model
model.fit(X_train, Y_train)

# Make predictions on the test set
Y_pred = model.predict(X_test)

# Evaluate accuracy
accuracy = accuracy_score(Y_test, Y_pred)
print(f"Accuracy: {accuracy}")

"""Légèrement mieux que l'autre mais dans le même ordre de grandeur."""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from time import time

start_time_total = time()

# Reshape X_grayscale to (18000, 32*32) as Logistic Regression expects 2D input
X_reshaped = X_grayscale.reshape((X_grayscale.shape[0], -1))

# Split the dataset into train and test sets
test_size = 0.2
X_train, X_test, Y_train, Y_test = train_test_split(X_reshaped, Y, test_size=test_size, random_state=42)

# Initialize the logistic regression model
model = LogisticRegression(max_iter=5000, solver='saga', random_state=42)

# Train the model
start_time_train = time()
model.fit(X_train, Y_train)
end_time_train = time()

# Make predictions on the test set
Y_pred = model.predict(X_test)

# Evaluate accuracy
accuracy = accuracy_score(Y_test, Y_pred)
print(f"Accuracy: {accuracy}")

# Accuracy on the training set
train_accuracy = model.score(X_train, Y_train)
print(f"Accuracy on the training set: {train_accuracy}")

# Accuracy on the test set
test_accuracy = model.score(X_test, Y_test)
print(f"Accuracy on the test set: {test_accuracy}")

# Times
end_time_total = time()
print(f"Total time: {end_time_total - start_time_total} seconds")
print(f"Train time: {end_time_train - start_time_train} seconds")

from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
from time import time

start_time_total = time()

# Reshape X_grayscale to (18000, 32*32) as Naive Bayes expects 2D input
X_reshaped = X_grayscale.reshape((X_grayscale.shape[0], -1))

# Split the dataset into train and test sets
test_size = 0.20
X_train, X_test, Y_train, Y_test = train_test_split(X_reshaped, Y, test_size=test_size, random_state=42)

# Initialize the Naive Bayes model
model = GaussianNB()

# Train the model
start_time_train = time()
model.fit(X_train, Y_train)
end_time_train = time()

# Make predictions on the test set
Y_pred = model.predict(X_test)

# Evaluate accuracy
accuracy = accuracy_score(Y_test, Y_pred)
print(f"Accuracy: {accuracy}")

# Accuracy on the training set
train_accuracy = model.score(X_train, Y_train)
print(f"Accuracy on the training set: {train_accuracy}")

# Accuracy on the test set
test_accuracy = model.score(X_test, Y_test)
print(f"Accuracy on the test set: {test_accuracy}")

# Times
end_time_total = time()
print(f"Total time: {end_time_total - start_time_total} seconds")
print(f"Train time: {end_time_train - start_time_train} seconds")

"""l'accuracy pour le dataset de test est meilleur que celui du train, mais cela s'inverse au augmentant la taille du dataset de test à 25% par exemple."""

from sklearn.decomposition import PCA
from time import time

start_time_total = time()

# Reshape X_grayscale to (18000, 32*32)
X_reshaped = X_grayscale.reshape((X_grayscale.shape[0], -1))

pca = PCA(n_components=200)
X_pca = pca.fit_transform(X_reshaped)

# Split the dataset into train and test sets
test_size = 0.20
X_train, X_test, Y_train, Y_test = train_test_split(X_pca, Y, test_size=test_size, random_state=42)

# Initialize the logistic regression model
model = LogisticRegression(max_iter=5000, solver='saga', random_state=42)

# Train the model
start_time_train = time()
model.fit(X_train, Y_train)
end_time_train = time()

# Make predictions on the test set
Y_pred = model.predict(X_test)

# Evaluate accuracy
accuracy = accuracy_score(Y_test, Y_pred)
print(f"Accuracy: {accuracy}")

# Accuracy on the training set
train_accuracy = model.score(X_train, Y_train)
print(f"Accuracy on the training set: {train_accuracy}")

# Accuracy on the test set
test_accuracy = model.score(X_test, Y_test)
print(f"Accuracy on the test set: {test_accuracy}")

# Times
end_time_total = time()
print(f"Total time: {end_time_total - start_time_total} seconds")
print(f"Train time: {end_time_train - start_time_train} seconds")

"""On obtient une meilleure accuracy (très légère) mais surtout on divise par plus de 300 le temps d'entraînement du modèle."""

from sklearn.decomposition import PCA
from time import time

start_time_total = time()

# Reshape X_grayscale to (18000, 32*32)
X_reshaped = X_grayscale.reshape((X_grayscale.shape[0], -1))

pca = PCA(n_components=200)
X_pca = pca.fit_transform(X_reshaped)

# Split the dataset into train and test sets
test_size = 0.20
X_train, X_test, Y_train, Y_test = train_test_split(X_pca, Y, test_size=test_size, random_state=42)

# Initialize the Naive Bayes model
model = GaussianNB()

# Train the model
start_time_train = time()
model.fit(X_train, Y_train)
end_time_train = time()

# Make predictions on the test set
Y_pred = model.predict(X_test)

# Evaluate accuracy
accuracy = accuracy_score(Y_test, Y_pred)
print(f"Accuracy: {accuracy}")

# Accuracy on the training set
train_accuracy = model.score(X_train, Y_train)
print(f"Accuracy on the training set: {train_accuracy}")

# Accuracy on the test set
test_accuracy = model.score(X_test, Y_test)
print(f"Accuracy on the test set: {test_accuracy}")

# Times
end_time_total = time()
print(f"Total time: {end_time_total - start_time_total} seconds")
print(f"Train time: {end_time_train - start_time_train} seconds")

"""On augmente ici aussi l'accuracy très légèrement, et le temps d'entraînement est divisé par 3, même s'il était déjà court, par contre au augmente le temps d'éxécution total car effectuer une PCA est plus long que le temps d'entraînement, c'est donc pas très intéressant ici.

# 3.2. Deep Learning / Multilayer Perception
"""

import tensorflow as tf

X_gray_train, X_gray_test, y_train, y_test = train_test_split(X_grayscale, Y, test_size=0.2)
X_gray_train = X_gray_train / 255.0
X_gray_test = X_gray_test / 255.0

Y_train = tf.keras.utils.to_categorical(y_train, num_classes=3)
Y_test = tf.keras.utils.to_categorical(y_test, num_classes=3)

model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(32, 32)),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dropout(0.65),
    tf.keras.layers.Dense(128, activation='relu'), #layer rajouté par la suite
    tf.keras.layers.Dropout(0.65), #layer rajouté par la suite
    tf.keras.layers.Dense(3, activation='Softmax')
])

model.compile(optimizer=tf.keras.optimizers.Adam(0.00013),
              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
              metrics=[tf.keras.metrics.CategoricalAccuracy()])
model.summary()

outputs = model.fit(X_gray_train, Y_train, epochs=70, batch_size = 256, validation_data=(X_gray_test, Y_test))

print("Train_loss:", outputs.history['loss'])
print("Val_loss:", outputs.history['val_loss'])
print("Train_acc:", outputs.history['categorical_accuracy'])
print("Val_acc:", outputs.history['val_categorical_accuracy'])

# Affiche le graphique de la perte
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(outputs.history['loss'], label='Train Loss')
plt.plot(outputs.history['val_loss'], label='Val Loss')
plt.title('Evolution de la Perte (Loss)')
plt.xlabel('Époque')
plt.ylabel('Perte')
plt.legend()

# Affiche le graphique de la précision
plt.subplot(1, 2, 2)
plt.plot(outputs.history['categorical_accuracy'], label='Train Accuracy')
plt.plot(outputs.history['val_categorical_accuracy'], label='Val Accuracy')
plt.title('Évolution de la Précision (Accuracy)')
plt.xlabel('Époque')
plt.ylabel('Précision')
plt.legend()

# Affiche les graphiques
plt.tight_layout()
plt.show()

"""# 3.3. Deep Learning / Convolutionnal Neural Network"""

from tensorflow.keras import datasets, layers, models

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)

X_train = X_train / 255.0
X_test = X_test / 255.0

Y_train = tf.keras.utils.to_categorical(y_train, num_classes=3)
Y_test = tf.keras.utils.to_categorical(y_test, num_classes=3)

# Créer le modèle CNN
model = models.Sequential()

# Première couche de convolution suivie d'une couche de pooling
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))
model.add(layers.MaxPooling2D((2, 2)))

# Deuxième couche de convolution suivie d'une couche de pooling
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))

# Troisième couche de convolution suivie d'une couche de pooling
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))

# Aplatir les données avant la couche dense
model.add(layers.Flatten())

# Couche dense
model.add(layers.Dense(64, activation='relu'))

# Couche de sortie
model.add(layers.Dense(3, activation='softmax'))

# Compiler le modèle
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Entraîner le modèle
history = model.fit(X_train, Y_train, epochs=20, validation_data=(X_test, Y_test))

# Créer deux sous-graphiques côte à côte
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

# Afficher le graphique de l'évolution de la loss
ax1.plot(history.history['loss'], label='Train Loss')
ax1.plot(history.history['val_loss'], label='Validation Loss')
ax1.set_title('Evolution de la Loss')
ax1.set_xlabel('Epochs')
ax1.set_ylabel('Loss')
ax1.legend()

# Afficher le graphique de l'évolution de l'accuracy
ax2.plot(history.history['accuracy'], label='Train Accuracy')
ax2.plot(history.history['val_accuracy'], label='Validation Accuracy')
ax2.set_title('Evolution de l\'Accuracy')
ax2.set_xlabel('Epochs')
ax2.set_ylabel('Accuracy')
ax2.legend()

# Afficher les graphiques
plt.show()

from tensorflow.keras import datasets, layers, models

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)

X_train = X_train / 255.0
X_test = X_test / 255.0

Y_train = tf.keras.utils.to_categorical(y_train, num_classes=3)
Y_test = tf.keras.utils.to_categorical(y_test, num_classes=3)

model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))
model.add(layers.Dropout(0.2))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.Dropout(0.2))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.Dropout(0.2))

model.summary()

model.add(layers.Flatten())
model.add(layers.Dense(128, activation='relu'))
model.add(layers.Dense(3))

model.summary()

model.compile(optimizer=tf.keras.optimizers.Adam(0.0001),
              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
              metrics=[tf.keras.metrics.CategoricalAccuracy()])

outputs = model.fit(X_train, Y_train, epochs=30, validation_data=(X_test, Y_test))

# Affiche le graphique de la perte
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(outputs.history['loss'], label='Train Loss')
plt.plot(outputs.history['val_loss'], label='Val Loss')
plt.title('Evolution de la Perte (Loss)')
plt.xlabel('Époque')
plt.ylabel('Perte')
plt.legend()

# Affiche le graphique de la précision
plt.subplot(1, 2, 2)
plt.plot(outputs.history['categorical_accuracy'], label='Train Accuracy')
plt.plot(outputs.history['val_categorical_accuracy'], label='Val Accuracy')
plt.title('Évolution de la Précision (Accuracy)')
plt.xlabel('Époque')
plt.ylabel('Précision')
plt.legend()

# Affiche les graphiques
plt.tight_layout()
plt.show()

from tensorflow.keras import datasets, layers, models

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)

X_train = X_train / 255.0
X_test = X_test / 255.0

Y_train = tf.keras.utils.to_categorical(y_train, num_classes=3)
Y_test = tf.keras.utils.to_categorical(y_test, num_classes=3)

model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))
model.add(layers.Dropout(0.17))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.Dropout(0.17))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.Dropout(0.17))


model.add(layers.Flatten())
model.add(layers.Dense(128, activation='relu'))
model.add(layers.Dense(3))


model.compile(optimizer=tf.keras.optimizers.Adam(0.00008),
              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
              metrics=[tf.keras.metrics.CategoricalAccuracy()])

outputs = model.fit(X_train, Y_train, epochs=40, validation_data=(X_test, Y_test))

# Affiche le graphique de la perte
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(outputs.history['loss'], label='Train Loss')
plt.plot(outputs.history['val_loss'], label='Val Loss')
plt.title('Evolution de la Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Affiche le graphique de la précision
plt.subplot(1, 2, 2)
plt.plot(outputs.history['categorical_accuracy'], label='Train Accuracy')
plt.plot(outputs.history['val_categorical_accuracy'], label='Validation Accuracy')
plt.title('Évolution de l\'Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Affiche les graphiques
plt.tight_layout()
plt.show()

from tensorflow.keras import datasets, layers, models
from sklearn.model_selection import train_test_split
import tensorflow as tf
import matplotlib.pyplot as plt

# Charger vos données (remplacez cette partie avec votre propre chargement de données)
# X, Y = ...

# Diviser les données en ensembles d'entraînement et de test
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20)

# Normaliser les valeurs de pixels entre 0 et 1
X_train = X_train / 255.0
X_test = X_test / 255.0

# Convertir les étiquettes en one-hot encoding
Y_train = tf.keras.utils.to_categorical(y_train, num_classes=3)
Y_test = tf.keras.utils.to_categorical(y_test, num_classes=3)

# Créer le modèle CNN avec des couches de dropout
model = models.Sequential()

model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))
model.add(layers.Dropout(0.17))
model.add(layers.MaxPooling2D((2, 2)))

model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.Dropout(0.17))
model.add(layers.MaxPooling2D((2, 2)))

model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.Dropout(0.17))
model.add(layers.MaxPooling2D((2, 2)))

model.add(layers.Flatten())

model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dropout(0.17))

model.add(layers.Dense(3, activation='softmax'))

# Utiliser un taux d'apprentissage plus bas pour Adam
custom_optimizer = tf.keras.optimizers.Adam(learning_rate=0.00008)

model.compile(optimizer=custom_optimizer,
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Entraîner le modèle avec 40 epochs
history = model.fit(X_train, Y_train, epochs=40, validation_data=(X_test, Y_test))

# Créer deux sous-graphiques côte à côte
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

# Afficher le graphique de l'évolution de la loss
ax1.plot(history.history['loss'], label='Train Loss')
ax1.plot(history.history['val_loss'], label='Validation Loss')
ax1.set_title('Evolution de la Loss')
ax1.set_xlabel('Epochs')
ax1.set_ylabel('Loss')
ax1.legend()

# Afficher le graphique de l'évolution de l'accuracy
ax2.plot(history.history['accuracy'], label='Train Accuracy')
ax2.plot(history.history['val_accuracy'], label='Validation Accuracy')
ax2.set_title('Evolution de l\'Accuracy')
ax2.set_xlabel('Epochs')
ax2.set_ylabel('Accuracy')
ax2.legend()

# Afficher les graphiques
plt.show()

